{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import prereq.\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler # used for callback \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = pd.read_csv('X.csv')\n",
    "Y = pd.read_csv('Y.csv')\n",
    "\n",
    "X_c = pd.read_csv('X_c.csv')\n",
    "Y_c = pd.read_csv('Y_c.csv')\n",
    "\n",
    "X_t = pd.read_csv('X_t.csv')\n",
    "Y_t = pd.read_csv('Y_t.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy  np.append(a, [[7,8,9]],axis = 0)\n",
    "data_type =  np.dtype(np.float64) # define datatype\n",
    "\n",
    "x = np.asarray(X.iloc[:,1:].to_numpy(), dtype=data_type)\n",
    "y = np.asarray(Y.iloc[:,1].to_numpy(), dtype=data_type)\n",
    "x_t = np.asarray(X_t.iloc[:,1:].to_numpy(), dtype=data_type)\n",
    "y_t = np.asarray(Y_t.iloc[:,1].to_numpy(), dtype=data_type)\n",
    "x_c = np.asarray(X_c.iloc[:,1:].to_numpy(), dtype=data_type)\n",
    "y_c = np.asarray(Y_c.iloc[:,1].to_numpy(), dtype=data_type)\n",
    "\n",
    "\n",
    "# clean some memory\n",
    "X = X_c = X_t = Y = Y_c = Y_t = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x.reshape(x.shape[0], img_rows, img_cols, 1)\n",
    "x_cross = x_c.reshape(x_c.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_t.reshape(x_t.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_cross = x_cross.astype('float32')\n",
    "x_test = x_test.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y, num_classes)\n",
    "y_cross = keras.utils.to_categorical(y_c, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_t, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.62 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.62 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.62 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.52 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "# arhitect the model\n",
    "# CGG - LITE 28x28 MAS\n",
    "model = tf.keras.Sequential([\n",
    "\n",
    "# layers.Conv2D(8, kernel_size=(7, 7),\n",
    "#                  padding='same',\n",
    "#                  activation='relu',\n",
    "#                  input_shape=input_shape),  \n",
    "# layers.Dropout(0.7),\n",
    "# # layers.Dropout(0.5),\n",
    "# # layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "# # layers.Conv2D(16, padding='same', kernel_size=(14, 14), activation='relu'),\n",
    "# # layers.Dropout(0.7),\n",
    "# # layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "# # layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "# # layers.Conv2D(16, padding='same',kernel_size=(1, 3), activation='relu'),\n",
    "# # layers.Dropout(0.45),\n",
    "    \n",
    "# layers.Flatten(),\n",
    "\n",
    "# # layers.Dense(1024, activation='relu'),#1024\n",
    "# # layers.Dropout(0.5),\n",
    "    \n",
    "# layers.Dense(3072, activation='relu'),#1024\n",
    "# layers.Dropout(0.6),\n",
    "    \n",
    "# layers.Dense(3072, activation='relu'),#1024\n",
    "# layers.Dropout(0.6),\n",
    "    \n",
    "\n",
    "# layers.Dense(400, activation='relu'),#1024\n",
    "# layers.Dropout(0.55),\n",
    "\n",
    "# layers.Dense(100, activation='relu'),#1024\n",
    "# layers.Dropout(0.5),\n",
    "    \n",
    "# layers.Dense(10, activation='softmax')]) -- best\n",
    "    \n",
    "    \n",
    "layers.Conv2D(10, kernel_size=(7, 7),\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape),  \n",
    "layers.Dropout(0.65),\n",
    "# layers.Dropout(0.5),\n",
    "# layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "# layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "# layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "# layers.Conv2D(16, padding='same',kernel_size=(1, 3), activation='relu'),\n",
    "# layers.Dropout(0.45),\n",
    "    \n",
    "layers.Flatten(),\n",
    "\n",
    "# layers.Dense(1024, activation='relu'),#1024\n",
    "# layers.Dropout(0.5),\n",
    "    \n",
    "layers.Dense(3072, activation='relu'),#1024\n",
    "layers.Dropout(0.62),\n",
    "    \n",
    "layers.Dense(3072, activation='relu'),#1024\n",
    "layers.Dropout(0.62),\n",
    "    \n",
    "layers.Dense(3072, activation='relu'),#1024\n",
    "layers.Dropout(0.62),\n",
    "      \n",
    "layers.Dense(400, activation='relu'),#1024\n",
    "layers.Dropout(0.52),\n",
    "    \n",
    "layers.Dense(100, activation='relu'),#1024\n",
    "layers.Dropout(0.52),\n",
    "        \n",
    "layers.Dense(10, activation='softmax')])\n",
    "# max 90  acc\n",
    "\n",
    "# 69300/69300 [==============================] - 13s 181us/sample - loss: 0.0626 - accuracy: 0.9767 - val_loss: 0.4450 - val_accuracy: 0.9143\n",
    "# Out[48]:\n",
    "# <tensorflow.python.keras.callbacks.History at 0x1349aba3a48>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.001\n",
    "    k = 1\n",
    "    return k / np.sqrt(epoch+1) * initial_lrate\n",
    "     \n",
    "\n",
    "lrate = LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "callbacks = [EarlyStopping(monitor='val_accuracy',verbose=1,min_delta=0.00000001, patience=90), lrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 277200 samples, validate on 1400 samples\n",
      "Epoch 1/100\n",
      "277200/277200 [==============================] - 128s 462us/sample - loss: 0.7981 - accuracy: 0.7126 - val_loss: 0.3391 - val_accuracy: 0.8786\n",
      "Epoch 2/100\n",
      "277200/277200 [==============================] - 126s 456us/sample - loss: 0.4893 - accuracy: 0.8373 - val_loss: 0.3367 - val_accuracy: 0.8943\n",
      "Epoch 3/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.4261 - accuracy: 0.8579 - val_loss: 0.2657 - val_accuracy: 0.9100\n",
      "Epoch 4/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.3880 - accuracy: 0.8694 - val_loss: 0.2599 - val_accuracy: 0.9043\n",
      "Epoch 5/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.3617 - accuracy: 0.8773 - val_loss: 0.2387 - val_accuracy: 0.9086\n",
      "Epoch 6/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.3410 - accuracy: 0.8841 - val_loss: 0.2427 - val_accuracy: 0.9086\n",
      "Epoch 7/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.3236 - accuracy: 0.8903 - val_loss: 0.2455 - val_accuracy: 0.9014\n",
      "Epoch 8/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.3086 - accuracy: 0.8941 - val_loss: 0.2266 - val_accuracy: 0.9086\n",
      "Epoch 9/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.2977 - accuracy: 0.8986 - val_loss: 0.2284 - val_accuracy: 0.9114\n",
      "Epoch 10/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.2866 - accuracy: 0.9021 - val_loss: 0.2317 - val_accuracy: 0.9100\n",
      "Epoch 11/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.2774 - accuracy: 0.9050 - val_loss: 0.2268 - val_accuracy: 0.9157\n",
      "Epoch 12/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.2679 - accuracy: 0.9090 - val_loss: 0.2247 - val_accuracy: 0.9129\n",
      "Epoch 13/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.2640 - accuracy: 0.9097 - val_loss: 0.2195 - val_accuracy: 0.9214\n",
      "Epoch 14/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.2509 - accuracy: 0.9140 - val_loss: 0.2163 - val_accuracy: 0.9157\n",
      "Epoch 15/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.2492 - accuracy: 0.9151 - val_loss: 0.2128 - val_accuracy: 0.9200\n",
      "Epoch 16/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.2371 - accuracy: 0.9185 - val_loss: 0.2118 - val_accuracy: 0.9143\n",
      "Epoch 17/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.2353 - accuracy: 0.9198 - val_loss: 0.2199 - val_accuracy: 0.9214\n",
      "Epoch 18/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.2279 - accuracy: 0.9223 - val_loss: 0.2171 - val_accuracy: 0.9271\n",
      "Epoch 19/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.2222 - accuracy: 0.9247 - val_loss: 0.2167 - val_accuracy: 0.9229\n",
      "Epoch 20/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.2180 - accuracy: 0.9259 - val_loss: 0.1984 - val_accuracy: 0.9243\n",
      "Epoch 21/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.2133 - accuracy: 0.9275 - val_loss: 0.2186 - val_accuracy: 0.9143\n",
      "Epoch 22/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.2109 - accuracy: 0.9285 - val_loss: 0.2225 - val_accuracy: 0.9171\n",
      "Epoch 23/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.2035 - accuracy: 0.9305 - val_loss: 0.2322 - val_accuracy: 0.9143\n",
      "Epoch 24/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1994 - accuracy: 0.9325 - val_loss: 0.2340 - val_accuracy: 0.9157\n",
      "Epoch 25/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1962 - accuracy: 0.9339 - val_loss: 0.2150 - val_accuracy: 0.9200\n",
      "Epoch 26/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1952 - accuracy: 0.9341 - val_loss: 0.2247 - val_accuracy: 0.9186\n",
      "Epoch 27/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1883 - accuracy: 0.9361 - val_loss: 0.2277 - val_accuracy: 0.9186\n",
      "Epoch 28/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1846 - accuracy: 0.9366 - val_loss: 0.2327 - val_accuracy: 0.9171\n",
      "Epoch 29/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1806 - accuracy: 0.9384 - val_loss: 0.2295 - val_accuracy: 0.9200\n",
      "Epoch 30/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1777 - accuracy: 0.9400 - val_loss: 0.2211 - val_accuracy: 0.9286\n",
      "Epoch 31/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1767 - accuracy: 0.9406 - val_loss: 0.2207 - val_accuracy: 0.9157\n",
      "Epoch 32/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1734 - accuracy: 0.9417 - val_loss: 0.2404 - val_accuracy: 0.9214\n",
      "Epoch 33/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1713 - accuracy: 0.9420 - val_loss: 0.2263 - val_accuracy: 0.9200\n",
      "Epoch 34/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1691 - accuracy: 0.9433 - val_loss: 0.2176 - val_accuracy: 0.9114\n",
      "Epoch 35/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1664 - accuracy: 0.9443 - val_loss: 0.2308 - val_accuracy: 0.9186\n",
      "Epoch 36/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1632 - accuracy: 0.9455 - val_loss: 0.2181 - val_accuracy: 0.9300\n",
      "Epoch 37/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1622 - accuracy: 0.9457 - val_loss: 0.2166 - val_accuracy: 0.9257\n",
      "Epoch 38/100\n",
      "277200/277200 [==============================] - 126s 456us/sample - loss: 0.1591 - accuracy: 0.9467 - val_loss: 0.2133 - val_accuracy: 0.9243\n",
      "Epoch 39/100\n",
      "277200/277200 [==============================] - 126s 456us/sample - loss: 0.1554 - accuracy: 0.9481 - val_loss: 0.2149 - val_accuracy: 0.9257\n",
      "Epoch 40/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1557 - accuracy: 0.9482 - val_loss: 0.2224 - val_accuracy: 0.9243\n",
      "Epoch 41/100\n",
      "277200/277200 [==============================] - 126s 456us/sample - loss: 0.1505 - accuracy: 0.9495 - val_loss: 0.2190 - val_accuracy: 0.9271\n",
      "Epoch 42/100\n",
      "277200/277200 [==============================] - 126s 456us/sample - loss: 0.1491 - accuracy: 0.9500 - val_loss: 0.2254 - val_accuracy: 0.9271\n",
      "Epoch 43/100\n",
      "277200/277200 [==============================] - 126s 456us/sample - loss: 0.1467 - accuracy: 0.9514 - val_loss: 0.2208 - val_accuracy: 0.9271\n",
      "Epoch 44/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1446 - accuracy: 0.9520 - val_loss: 0.2248 - val_accuracy: 0.9157\n",
      "Epoch 45/100\n",
      "277200/277200 [==============================] - 126s 456us/sample - loss: 0.1420 - accuracy: 0.9531 - val_loss: 0.2197 - val_accuracy: 0.9229\n",
      "Epoch 46/100\n",
      "277200/277200 [==============================] - 126s 456us/sample - loss: 0.1412 - accuracy: 0.9528 - val_loss: 0.2184 - val_accuracy: 0.9257\n",
      "Epoch 47/100\n",
      "277200/277200 [==============================] - 126s 456us/sample - loss: 0.1402 - accuracy: 0.9540 - val_loss: 0.2350 - val_accuracy: 0.9257\n",
      "Epoch 48/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1353 - accuracy: 0.9553 - val_loss: 0.2242 - val_accuracy: 0.9314\n",
      "Epoch 49/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1359 - accuracy: 0.9549 - val_loss: 0.2292 - val_accuracy: 0.9257\n",
      "Epoch 50/100\n",
      "277200/277200 [==============================] - 126s 456us/sample - loss: 0.1337 - accuracy: 0.9555 - val_loss: 0.2427 - val_accuracy: 0.9329\n",
      "Epoch 51/100\n",
      "277200/277200 [==============================] - 126s 456us/sample - loss: 0.1345 - accuracy: 0.9554 - val_loss: 0.2372 - val_accuracy: 0.9271\n",
      "Epoch 52/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1317 - accuracy: 0.9568 - val_loss: 0.2280 - val_accuracy: 0.9329\n",
      "Epoch 53/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1313 - accuracy: 0.9572 - val_loss: 0.2369 - val_accuracy: 0.9286\n",
      "Epoch 54/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1284 - accuracy: 0.9580 - val_loss: 0.2178 - val_accuracy: 0.9271\n",
      "Epoch 55/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1273 - accuracy: 0.9585 - val_loss: 0.2267 - val_accuracy: 0.9329\n",
      "Epoch 56/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1257 - accuracy: 0.9589 - val_loss: 0.2273 - val_accuracy: 0.9314\n",
      "Epoch 57/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1248 - accuracy: 0.9592 - val_loss: 0.2282 - val_accuracy: 0.9343\n",
      "Epoch 58/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1230 - accuracy: 0.9598 - val_loss: 0.2231 - val_accuracy: 0.9343\n",
      "Epoch 59/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1203 - accuracy: 0.9606 - val_loss: 0.2318 - val_accuracy: 0.9271\n",
      "Epoch 60/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1216 - accuracy: 0.9608 - val_loss: 0.2170 - val_accuracy: 0.9343\n",
      "Epoch 61/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1193 - accuracy: 0.9616 - val_loss: 0.2387 - val_accuracy: 0.9286\n",
      "Epoch 62/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1173 - accuracy: 0.9615 - val_loss: 0.2209 - val_accuracy: 0.9343\n",
      "Epoch 63/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1180 - accuracy: 0.9619 - val_loss: 0.2274 - val_accuracy: 0.9300\n",
      "Epoch 64/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1158 - accuracy: 0.9630 - val_loss: 0.2081 - val_accuracy: 0.9329\n",
      "Epoch 65/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1147 - accuracy: 0.9630 - val_loss: 0.2426 - val_accuracy: 0.9329\n",
      "Epoch 66/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1120 - accuracy: 0.9636 - val_loss: 0.2094 - val_accuracy: 0.9300\n",
      "Epoch 67/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1113 - accuracy: 0.9638 - val_loss: 0.2117 - val_accuracy: 0.9314\n",
      "Epoch 68/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1110 - accuracy: 0.9645 - val_loss: 0.2186 - val_accuracy: 0.9357\n",
      "Epoch 69/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1082 - accuracy: 0.9648 - val_loss: 0.2229 - val_accuracy: 0.9314\n",
      "Epoch 70/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1084 - accuracy: 0.9654 - val_loss: 0.2359 - val_accuracy: 0.9343\n",
      "Epoch 71/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1090 - accuracy: 0.9651 - val_loss: 0.2241 - val_accuracy: 0.9300\n",
      "Epoch 72/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1057 - accuracy: 0.9659 - val_loss: 0.2460 - val_accuracy: 0.9357\n",
      "Epoch 73/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1064 - accuracy: 0.9662 - val_loss: 0.2465 - val_accuracy: 0.9329\n",
      "Epoch 74/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1062 - accuracy: 0.9667 - val_loss: 0.2439 - val_accuracy: 0.9329\n",
      "Epoch 75/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1032 - accuracy: 0.9668 - val_loss: 0.2377 - val_accuracy: 0.9286\n",
      "Epoch 76/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1040 - accuracy: 0.9669 - val_loss: 0.2281 - val_accuracy: 0.9329\n",
      "Epoch 77/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1015 - accuracy: 0.9670 - val_loss: 0.2394 - val_accuracy: 0.9371\n",
      "Epoch 78/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.1006 - accuracy: 0.9681 - val_loss: 0.2466 - val_accuracy: 0.9314\n",
      "Epoch 79/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.0986 - accuracy: 0.9682 - val_loss: 0.2290 - val_accuracy: 0.9357\n",
      "Epoch 80/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.0989 - accuracy: 0.9683 - val_loss: 0.2440 - val_accuracy: 0.9271\n",
      "Epoch 81/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.0997 - accuracy: 0.9682 - val_loss: 0.2465 - val_accuracy: 0.9314\n",
      "Epoch 82/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.0993 - accuracy: 0.9686 - val_loss: 0.2537 - val_accuracy: 0.9357\n",
      "Epoch 83/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.0981 - accuracy: 0.9689 - val_loss: 0.2341 - val_accuracy: 0.9314\n",
      "Epoch 84/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.0949 - accuracy: 0.9697 - val_loss: 0.2428 - val_accuracy: 0.9286\n",
      "Epoch 85/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.0967 - accuracy: 0.9695 - val_loss: 0.2415 - val_accuracy: 0.9343\n",
      "Epoch 86/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.0960 - accuracy: 0.9698 - val_loss: 0.2347 - val_accuracy: 0.9371\n",
      "Epoch 87/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.0950 - accuracy: 0.9701 - val_loss: 0.2536 - val_accuracy: 0.9343\n",
      "Epoch 88/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.0930 - accuracy: 0.9708 - val_loss: 0.2438 - val_accuracy: 0.9329\n",
      "Epoch 89/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.0919 - accuracy: 0.9710 - val_loss: 0.2221 - val_accuracy: 0.9271\n",
      "Epoch 90/100\n",
      "277200/277200 [==============================] - 126s 456us/sample - loss: 0.0937 - accuracy: 0.9703 - val_loss: 0.2232 - val_accuracy: 0.9329\n",
      "Epoch 91/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.0912 - accuracy: 0.9713 - val_loss: 0.2566 - val_accuracy: 0.9271\n",
      "Epoch 92/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.0894 - accuracy: 0.9716 - val_loss: 0.2455 - val_accuracy: 0.9329\n",
      "Epoch 93/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.0903 - accuracy: 0.9715 - val_loss: 0.2447 - val_accuracy: 0.9371\n",
      "Epoch 94/100\n",
      "277200/277200 [==============================] - 126s 456us/sample - loss: 0.0879 - accuracy: 0.9720 - val_loss: 0.2382 - val_accuracy: 0.9314\n",
      "Epoch 95/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.0870 - accuracy: 0.9727 - val_loss: 0.2483 - val_accuracy: 0.9357\n",
      "Epoch 96/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.0868 - accuracy: 0.9724 - val_loss: 0.2474 - val_accuracy: 0.9371\n",
      "Epoch 97/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.0873 - accuracy: 0.9729 - val_loss: 0.2529 - val_accuracy: 0.9386\n",
      "Epoch 98/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.0889 - accuracy: 0.9724 - val_loss: 0.2355 - val_accuracy: 0.9371\n",
      "Epoch 99/100\n",
      "277200/277200 [==============================] - 126s 456us/sample - loss: 0.0854 - accuracy: 0.9731 - val_loss: 0.2457 - val_accuracy: 0.9386\n",
      "Epoch 100/100\n",
      "277200/277200 [==============================] - 126s 455us/sample - loss: 0.0859 - accuracy: 0.9733 - val_loss: 0.2398 - val_accuracy: 0.9343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x216fa5a2348>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train, \n",
    "          epochs=100,\n",
    "          batch_size=100,\n",
    "          shuffle=True, \n",
    "          verbose=1,\n",
    "         validation_data=(x_cross, y_cross), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "350/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 209us/sample - loss: 0.2197 - accuracy: 0.9314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2505875767128808, 0.93142855]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('97_935_93.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
