# fashion-mnist-cnn

At the begining, I started blind trying some lite version of VGG (some derivation of some deeper layers in VGG architecture), but then i developed a new strategy, i got a new, better learning rate = 0.00018, and from my best arhtiecture on this dataset without augmentation (4 C 7X7 Same Dropout0.6 -> 2 D 2048 Dropout0.5 -> 1 D 200 Dropout) i got on train set ACC 93.82% and on devset i got ACC 91.2%. There was a porblem of bias and varaince, but this arhitect was the best one.
Then i tryed to achive more and after i augmented the dataset with negative photos, photos with value of pixel / 2 and negative photos with pixel values / 2 (cuz i want my net to recogniez the shape of the objects), from my best arhtiecture 
(that can be found in interative_model_dent), got ACC on train = 97.33%, on dev set got ACC = 93.43% and on test set got 93.14%. On this arhtiecture i tryed to retrain it (on try_to_achive_more_with97ac),  just on original trainign datasaet, and i used some special callbacks (EarlyStopping, LearningRateScheduler, ModelCheckpoint), but that dosen't achive that much that i wanted. 
After a few attempts, i put all i got my best on model_lastIterationV2 (callback: EarlyStopping, LearningRateScheduler, ModelCheckpoint), BatchNormalization (with momentum of 0.99 to average over 100 values, cuz i chosed minimatch of 100), Adam(with beta1 = 0.99, to average over 100 values). I tryed a lot of arhitectures, but my best was almost 94% acc on test and dev set.
(PS: i tryed keras.preprocessing.image.ImageDataGenerator, but this wasn't helping, was just comutational expesiv).
In my point of view, 94% acc on this dataset could be well over human-level performance, and on the oficial repo on github, (https://github.com/zalandoresearch/fashion-mnist) somebody got 'Crowd-sourced evaluation of human (with no fashion expertise) performance. 1000 randomly sampled test images, 3 labels per image, majority labelling.' - ACC: 83.5%.
