{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import prereq.\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint # used for callback \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = pd.read_csv('X.csv')\n",
    "Y = pd.read_csv('Y.csv')\n",
    "\n",
    "X_c = pd.read_csv('X_c.csv')\n",
    "Y_c = pd.read_csv('Y_c.csv')\n",
    "\n",
    "X_t = pd.read_csv('X_t.csv')\n",
    "Y_t = pd.read_csv('Y_t.csv')\n",
    "\n",
    "XX_c = X_c.iloc[0:int(X.shape[0]*1/4),:]\n",
    "YY_c = Y_c.iloc[0:int(X.shape[0]*1/4),:]\n",
    "\n",
    "X_train = X.iloc[0:int(X.shape[0]*1/4-1),:]\n",
    "Y_train = Y.iloc[0:int(X.shape[0]*1/4-1),:]\n",
    "\n",
    "X_pretrain = X.iloc[int(X.shape[0]*1/4):int(X.shape[0]),:]\n",
    "Y_pretrain = Y.iloc[int(X.shape[0]*1/4):int(X.shape[0]),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen =  tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            zoom_range=0.05,\n",
    "            width_shift_range=0.05,\n",
    "            height_shift_range=0.05,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode=\"reflect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy  np.append(a, [[7,8,9]],axis = 0)\n",
    "data_type =  np.dtype(np.float64) # define datatype\n",
    "\n",
    "x_Pretrain = np.asarray(X_pretrain.iloc[:,1:].to_numpy(), dtype=data_type)\n",
    "y_Pretrain = np.asarray(Y_pretrain.iloc[:,1].to_numpy(), dtype=data_type)\n",
    "\n",
    "x_Train = np.asarray(X.iloc[:,1:].to_numpy(), dtype=data_type)\n",
    "y_Train = np.asarray(Y.iloc[:,1].to_numpy(), dtype=data_type)\n",
    "\n",
    "x_t = np.asarray(X_t.iloc[:,1:].to_numpy(), dtype=data_type)\n",
    "y_t = np.asarray(Y_t.iloc[:,1].to_numpy(), dtype=data_type)\n",
    "x_c = np.asarray(XX_c.iloc[:,1:].to_numpy(), dtype=data_type)\n",
    "y_c = np.asarray(YY_c.iloc[:,1].to_numpy(), dtype=data_type)\n",
    "\n",
    "\n",
    "# clean some memory\n",
    "X = X_c = X_t = Y = Y_c = Y_t = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_Train.reshape(x_Train.shape[0], img_rows, img_cols, 1)\n",
    "x_pretrain = x_Pretrain.reshape(x_Pretrain.shape[0], img_rows, img_cols, 1)\n",
    "x_cross = x_c.reshape(x_c.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_t.reshape(x_t.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols,1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_pretrain = x_train.astype('float32')\n",
    "x_cross = x_cross.astype('float32')\n",
    "x_test = x_test.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_Train, num_classes)\n",
    "y_pretrain = keras.utils.to_categorical(y_Pretrain, num_classes)\n",
    "y_cross = keras.utils.to_categorical(y_c, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_t, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # arhitect the model\n",
    "# # lastIterationV2\n",
    "# model = tf.keras.Sequential([\n",
    "\n",
    "    \n",
    "# layers.Conv2D(4, kernel_size=(7, 7),\n",
    "#                  padding='same',\n",
    "#                  activation='relu',\n",
    "#                  input_shape=input_shape),  \n",
    "# layers.Dropout(0.1),\n",
    "# layers.Conv2D(8, padding='same', kernel_size=(7, 7), activation='relu'),\n",
    "# layers.Dropout(0.1),\n",
    "\n",
    "# layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "# layers.Conv2D(16, padding='same', kernel_size=(7, 7), activation='relu'),\n",
    "# layers.Dropout(0.1),\n",
    "# layers.Conv2D(16, padding='same', kernel_size=(7, 7), activation='relu'),\n",
    "# layers.Dropout(0.1),\n",
    "\n",
    "# layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "# layers.Conv2D(32, padding='same', kernel_size=(7, 7), activation='relu'),\n",
    "# layers.Dropout(0.1),\n",
    "# layers.Conv2D(32, padding='same', kernel_size=(7, 7), activation='relu'),\n",
    "# layers.Dropout(0.1),\n",
    "    \n",
    "# layers.Flatten(),\n",
    "\n",
    "# layers.Dense(1024, activation='relu'),#1024\n",
    "# layers.Dropout(0.1),\n",
    "    \n",
    "# layers.Dense(1024, activation='relu'),#1024\n",
    "# layers.Dropout(0.1),\n",
    "    \n",
    "# layers.Dense(1024, activation='relu'),#1024\n",
    "# layers.Dropout(0.1),\n",
    "      \n",
    "# layers.Dense(512, activation='relu'),#1024\n",
    "# layers.Dropout(0.1),\n",
    "    \n",
    "# layers.Dense(100, activation='relu'),#1024\n",
    "# layers.Dropout(0.1),\n",
    "        \n",
    "# layers.Dense(10, activation='softmax')])\n",
    "\n",
    "\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "\n",
    "    \n",
    "# layers.Conv2D(28, kernel_size=(3, 3),\n",
    "#                  padding='same',\n",
    "#                  activation='relu',\n",
    "#                  input_shape=input_shape),  \n",
    "# layers.BatchNormalization(momentum=0.99),\n",
    "# layers.Dropout(0.25),\n",
    "    \n",
    "# layers.Conv2D(28, padding='same', kernel_size=(3, 3), activation='relu'),\n",
    "# layers.BatchNormalization(momentum=0.99),\n",
    "# layers.Dropout(0.25),        \n",
    "    \n",
    "# layers.Conv2D(28, padding='same', kernel_size=(3, 3), activation='relu'),\n",
    "# layers.BatchNormalization(momentum=0.99),\n",
    "# layers.Dropout(0.25),    \n",
    "\n",
    "# layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "# layers.Conv2D(56, padding='same', kernel_size=(3, 3), activation='relu'),\n",
    "# layers.BatchNormalization(momentum=0.99),\n",
    "# layers.Dropout(0.25),   \n",
    "    \n",
    "# layers.Conv2D(56, padding='same', kernel_size=(3, 3), activation='relu'),\n",
    "# layers.BatchNormalization(momentum=0.99),\n",
    "# layers.Dropout(0.25),   \n",
    "    \n",
    "# layers.Conv2D(56, padding='same', kernel_size=(3, 3), activation='relu'),\n",
    "# layers.BatchNormalization(momentum=0.99),\n",
    "# layers.Dropout(0.25),   \n",
    "    \n",
    "# layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "# layers.Flatten(),\n",
    "\n",
    "# layers.Dense(700, activation='relu'),\n",
    "# layers.BatchNormalization(momentum=0.99),\n",
    "# layers.Dropout(0.2),\n",
    "    \n",
    "# layers.Dense(700, activation='relu'),\n",
    "# layers.BatchNormalization(momentum=0.99),\n",
    "# layers.Dropout(0.2),\n",
    "    \n",
    "# layers.Dense(200, activation='relu'),\n",
    "# layers.BatchNormalization(momentum=0.99),\n",
    "# layers.Dropout(0.1),\n",
    "    \n",
    "# layers.Dense(60, activation='relu'),\n",
    "# layers.BatchNormalization(momentum=0.99),\n",
    "# layers.Dropout(0.05),\n",
    "        \n",
    "# layers.Dense(10, activation='softmax')])\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([#retry 97 acc but with lower dropout\n",
    "layers.Conv2D(4, kernel_size=(4, 4), # 95.7 93.5\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape),  \n",
    "layers.BatchNormalization(momentum=0.99),\n",
    "layers.Dropout(0.5),\n",
    "# layers.Dropout(0.5),\n",
    "# layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "# layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "# layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "# layers.Conv2D(16, padding='same',kernel_size=(1, 3), activation='relu'),\n",
    "# layers.Dropout(0.45),\n",
    "    \n",
    "layers.Flatten(),\n",
    "\n",
    "# layers.Dense(1024, activation='relu'),#1024\n",
    "# layers.Dropout(0.5),\n",
    "    \n",
    "layers.Dense(1024, activation='relu'),#1024 best ----- last 94\n",
    "layers.BatchNormalization(momentum=0.99),\n",
    "layers.Dropout(0.4),\n",
    "    \n",
    "layers.Dense(200, activation='relu'),#1024\n",
    "layers.BatchNormalization(momentum=0.99),\n",
    "layers.Dropout(0.35),  \n",
    "        \n",
    "layers.Dense(10, activation='softmax')])\n",
    "\n",
    "# model = tf.keras.Sequential([ # 99.99 cu 93.5\n",
    "\n",
    "    \n",
    "# layers.Conv2D(8, kernel_size=(7, 7),\n",
    "#                  padding='same',\n",
    "#                  activation='relu',\n",
    "#                  input_shape=input_shape),  \n",
    "# layers.BatchNormalization(momentum=0.99),\n",
    "# layers.Dropout(0.25),\n",
    "    \n",
    "# layers.Flatten(),\n",
    "\n",
    "# layers.Dense(2048, activation='relu'),\n",
    "# layers.BatchNormalization(momentum=0.99),\n",
    "# layers.Dropout(0.2),\n",
    "    \n",
    "# layers.Dense(2048, activation='relu'),\n",
    "# layers.BatchNormalization(momentum=0.99),\n",
    "# layers.Dropout(0.2),\n",
    "    \n",
    "# layers.Dense(200, activation='relu'),\n",
    "# layers.BatchNormalization(momentum=0.99),\n",
    "# layers.Dropout(0.15),\n",
    "        \n",
    "#layers.Dense(10, activation='softmax')])\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "\n",
    "    \n",
    "# layers.Conv2D(4, kernel_size=(7, 7),\n",
    "#                  padding='same',\n",
    "#                  activation='relu',\n",
    "#                  input_shape=input_shape),  \n",
    "# layers.BatchNormalization(momentum=0.99),\n",
    "# layers.Dropout(0.25),\n",
    "    \n",
    "# layers.Flatten(),\n",
    "\n",
    "# layers.Dense(1024, activation='relu'),\n",
    "# layers.BatchNormalization(momentum=0.99),\n",
    "# layers.Dropout(0.2),\n",
    "    \n",
    "# layers.Dense(1024, activation='relu'),\n",
    "# layers.BatchNormalization(momentum=0.99),\n",
    "# layers.Dropout(0.2),\n",
    "    \n",
    "# layers.Dense(100, activation='relu'),\n",
    "# layers.BatchNormalization(momentum=0.99),\n",
    "# layers.Dropout(0.15),\n",
    "\n",
    "# layers.Dense(10, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.00018\n",
    "    return 1 / np.sqrt(epoch + 1) * initial_lrate\n",
    "     \n",
    "def step_decay_train(epoch):\n",
    "    initial_lrate = 0.00018\n",
    "    return 0.22 / np.sqrt(epoch + 1) * initial_lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(beta_1=0.99), # for avg over 100 values\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "mc = ModelCheckpoint('lastIterationV2finnalmodelll_best.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)\n",
    "\n",
    "callbacks = [es, mc,lrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 277200 samples, validate on 1400 samples\n",
      "Epoch 1/100\n",
      "276800/277200 [============================>.] - ETA: 0s - loss: 0.6003 - accuracy: 0.7873\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.88571, saving model to lastIterationV2finnalmodelll_best.h5\n",
      "277200/277200 [==============================] - 24s 88us/sample - loss: 0.6002 - accuracy: 0.7873 - val_loss: 0.2847 - val_accuracy: 0.8857\n",
      "Epoch 2/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.4006 - accuracy: 0.8544\n",
      "Epoch 00002: val_accuracy improved from 0.88571 to 0.91000, saving model to lastIterationV2finnalmodelll_best.h5\n",
      "277200/277200 [==============================] - 22s 81us/sample - loss: 0.4005 - accuracy: 0.8545 - val_loss: 0.2370 - val_accuracy: 0.9100\n",
      "Epoch 3/100\n",
      "277100/277200 [============================>.] - ETA: 0s - loss: 0.3516 - accuracy: 0.8715\n",
      "Epoch 00003: val_accuracy improved from 0.91000 to 0.92143, saving model to lastIterationV2finnalmodelll_best.h5\n",
      "277200/277200 [==============================] - 22s 80us/sample - loss: 0.3517 - accuracy: 0.8715 - val_loss: 0.2222 - val_accuracy: 0.9214\n",
      "Epoch 4/100\n",
      "276700/277200 [============================>.] - ETA: 0s - loss: 0.3221 - accuracy: 0.8823\n",
      "Epoch 00004: val_accuracy did not improve from 0.92143\n",
      "277200/277200 [==============================] - 22s 81us/sample - loss: 0.3221 - accuracy: 0.8822 - val_loss: 0.2167 - val_accuracy: 0.9157\n",
      "Epoch 5/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.3038 - accuracy: 0.8887\n",
      "Epoch 00005: val_accuracy improved from 0.92143 to 0.92714, saving model to lastIterationV2finnalmodelll_best.h5\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.3038 - accuracy: 0.8887 - val_loss: 0.2116 - val_accuracy: 0.9271\n",
      "Epoch 6/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.2874 - accuracy: 0.8945\n",
      "Epoch 00006: val_accuracy did not improve from 0.92714\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.2874 - accuracy: 0.8945 - val_loss: 0.2084 - val_accuracy: 0.9157\n",
      "Epoch 7/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.2734 - accuracy: 0.8994\n",
      "Epoch 00007: val_accuracy did not improve from 0.92714\n",
      "277200/277200 [==============================] - 22s 80us/sample - loss: 0.2735 - accuracy: 0.8994 - val_loss: 0.2056 - val_accuracy: 0.9229\n",
      "Epoch 8/100\n",
      "276700/277200 [============================>.] - ETA: 0s - loss: 0.2620 - accuracy: 0.9034\n",
      "Epoch 00008: val_accuracy improved from 0.92714 to 0.93000, saving model to lastIterationV2finnalmodelll_best.h5\n",
      "277200/277200 [==============================] - 22s 80us/sample - loss: 0.2619 - accuracy: 0.9035 - val_loss: 0.1995 - val_accuracy: 0.9300\n",
      "Epoch 9/100\n",
      "277100/277200 [============================>.] - ETA: 0s - loss: 0.2532 - accuracy: 0.9062\n",
      "Epoch 00009: val_accuracy improved from 0.93000 to 0.93286, saving model to lastIterationV2finnalmodelll_best.h5\n",
      "277200/277200 [==============================] - 22s 80us/sample - loss: 0.2532 - accuracy: 0.9062 - val_loss: 0.1978 - val_accuracy: 0.9329\n",
      "Epoch 10/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.2446 - accuracy: 0.9099\n",
      "Epoch 00010: val_accuracy did not improve from 0.93286\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.2446 - accuracy: 0.9099 - val_loss: 0.1974 - val_accuracy: 0.9271\n",
      "Epoch 11/100\n",
      "276500/277200 [============================>.] - ETA: 0s - loss: 0.2378 - accuracy: 0.9122\n",
      "Epoch 00011: val_accuracy did not improve from 0.93286\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.2378 - accuracy: 0.9122 - val_loss: 0.1998 - val_accuracy: 0.9314\n",
      "Epoch 12/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.2311 - accuracy: 0.9143\n",
      "Epoch 00012: val_accuracy did not improve from 0.93286\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.2310 - accuracy: 0.9144 - val_loss: 0.1985 - val_accuracy: 0.9286\n",
      "Epoch 13/100\n",
      "276500/277200 [============================>.] - ETA: 0s - loss: 0.2241 - accuracy: 0.9170\n",
      "Epoch 00013: val_accuracy improved from 0.93286 to 0.93429, saving model to lastIterationV2finnalmodelll_best.h5\n",
      "277200/277200 [==============================] - 22s 80us/sample - loss: 0.2241 - accuracy: 0.9170 - val_loss: 0.1951 - val_accuracy: 0.9343\n",
      "Epoch 14/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.2193 - accuracy: 0.9182\n",
      "Epoch 00014: val_accuracy did not improve from 0.93429\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.2193 - accuracy: 0.9182 - val_loss: 0.1934 - val_accuracy: 0.9329\n",
      "Epoch 15/100\n",
      "277100/277200 [============================>.] - ETA: 0s - loss: 0.2142 - accuracy: 0.9200\n",
      "Epoch 00015: val_accuracy improved from 0.93429 to 0.93571, saving model to lastIterationV2finnalmodelll_best.h5\n",
      "277200/277200 [==============================] - 22s 80us/sample - loss: 0.2142 - accuracy: 0.9200 - val_loss: 0.1916 - val_accuracy: 0.9357\n",
      "Epoch 16/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.2079 - accuracy: 0.9221\n",
      "Epoch 00016: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.2079 - accuracy: 0.9220 - val_loss: 0.1902 - val_accuracy: 0.9329\n",
      "Epoch 17/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.2030 - accuracy: 0.9242\n",
      "Epoch 00017: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.2030 - accuracy: 0.9242 - val_loss: 0.1979 - val_accuracy: 0.9271\n",
      "Epoch 18/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.2004 - accuracy: 0.9251\n",
      "Epoch 00018: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.2004 - accuracy: 0.9251 - val_loss: 0.1905 - val_accuracy: 0.9229\n",
      "Epoch 19/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.1947 - accuracy: 0.9275\n",
      "Epoch 00019: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1946 - accuracy: 0.9275 - val_loss: 0.1931 - val_accuracy: 0.9286\n",
      "Epoch 20/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.1918 - accuracy: 0.9285\n",
      "Epoch 00020: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1918 - accuracy: 0.9285 - val_loss: 0.1967 - val_accuracy: 0.9314\n",
      "Epoch 21/100\n",
      "276500/277200 [============================>.] - ETA: 0s - loss: 0.1878 - accuracy: 0.9298\n",
      "Epoch 00021: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1879 - accuracy: 0.9298 - val_loss: 0.1963 - val_accuracy: 0.9329\n",
      "Epoch 22/100\n",
      "276500/277200 [============================>.] - ETA: 0s - loss: 0.1843 - accuracy: 0.9315\n",
      "Epoch 00022: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1843 - accuracy: 0.9315 - val_loss: 0.1911 - val_accuracy: 0.9314\n",
      "Epoch 23/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.1806 - accuracy: 0.9329\n",
      "Epoch 00023: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1807 - accuracy: 0.9329 - val_loss: 0.1924 - val_accuracy: 0.9314\n",
      "Epoch 24/100\n",
      "276800/277200 [============================>.] - ETA: 0s - loss: 0.1787 - accuracy: 0.9331\n",
      "Epoch 00024: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 23s 81us/sample - loss: 0.1787 - accuracy: 0.9331 - val_loss: 0.1981 - val_accuracy: 0.9314\n",
      "Epoch 25/100\n",
      "277000/277200 [============================>.] - ETA: 0s - loss: 0.1763 - accuracy: 0.9339\n",
      "Epoch 00025: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 80us/sample - loss: 0.1763 - accuracy: 0.9339 - val_loss: 0.1955 - val_accuracy: 0.9329\n",
      "Epoch 26/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.1721 - accuracy: 0.9355\n",
      "Epoch 00026: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1721 - accuracy: 0.9355 - val_loss: 0.2005 - val_accuracy: 0.9343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "277000/277200 [============================>.] - ETA: 0s - loss: 0.1702 - accuracy: 0.9363\n",
      "Epoch 00027: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1702 - accuracy: 0.9363 - val_loss: 0.1942 - val_accuracy: 0.9314\n",
      "Epoch 28/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.1692 - accuracy: 0.9368\n",
      "Epoch 00028: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1692 - accuracy: 0.9368 - val_loss: 0.1965 - val_accuracy: 0.9314\n",
      "Epoch 29/100\n",
      "276800/277200 [============================>.] - ETA: 0s - loss: 0.1649 - accuracy: 0.9382\n",
      "Epoch 00029: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 80us/sample - loss: 0.1649 - accuracy: 0.9382 - val_loss: 0.1987 - val_accuracy: 0.9343\n",
      "Epoch 30/100\n",
      "276500/277200 [============================>.] - ETA: 0s - loss: 0.1606 - accuracy: 0.9401\n",
      "Epoch 00030: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1606 - accuracy: 0.9402 - val_loss: 0.2002 - val_accuracy: 0.9357\n",
      "Epoch 31/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.1596 - accuracy: 0.9403\n",
      "Epoch 00031: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1596 - accuracy: 0.9403 - val_loss: 0.2016 - val_accuracy: 0.9357\n",
      "Epoch 32/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.1585 - accuracy: 0.9408\n",
      "Epoch 00032: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1585 - accuracy: 0.9408 - val_loss: 0.2011 - val_accuracy: 0.9357\n",
      "Epoch 33/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.1570 - accuracy: 0.9413\n",
      "Epoch 00033: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1570 - accuracy: 0.9413 - val_loss: 0.2006 - val_accuracy: 0.9300\n",
      "Epoch 34/100\n",
      "276500/277200 [============================>.] - ETA: 0s - loss: 0.1545 - accuracy: 0.9424\n",
      "Epoch 00034: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1545 - accuracy: 0.9424 - val_loss: 0.2003 - val_accuracy: 0.9329\n",
      "Epoch 35/100\n",
      "276500/277200 [============================>.] - ETA: 0s - loss: 0.1516 - accuracy: 0.9432\n",
      "Epoch 00035: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1516 - accuracy: 0.9432 - val_loss: 0.1995 - val_accuracy: 0.9314\n",
      "Epoch 36/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.1490 - accuracy: 0.9438\n",
      "Epoch 00036: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1490 - accuracy: 0.9438 - val_loss: 0.2049 - val_accuracy: 0.9343\n",
      "Epoch 37/100\n",
      "276500/277200 [============================>.] - ETA: 0s - loss: 0.1488 - accuracy: 0.9441\n",
      "Epoch 00037: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1488 - accuracy: 0.9441 - val_loss: 0.2044 - val_accuracy: 0.9329\n",
      "Epoch 38/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.1465 - accuracy: 0.9452\n",
      "Epoch 00038: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1465 - accuracy: 0.9452 - val_loss: 0.2061 - val_accuracy: 0.9329\n",
      "Epoch 39/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.1446 - accuracy: 0.9461\n",
      "Epoch 00039: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1447 - accuracy: 0.9461 - val_loss: 0.2062 - val_accuracy: 0.9314\n",
      "Epoch 40/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.1425 - accuracy: 0.9471\n",
      "Epoch 00040: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1425 - accuracy: 0.9471 - val_loss: 0.2044 - val_accuracy: 0.9329\n",
      "Epoch 41/100\n",
      "276500/277200 [============================>.] - ETA: 0s - loss: 0.1416 - accuracy: 0.9468\n",
      "Epoch 00041: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1415 - accuracy: 0.9468 - val_loss: 0.2039 - val_accuracy: 0.9314\n",
      "Epoch 42/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.1410 - accuracy: 0.9472\n",
      "Epoch 00042: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1410 - accuracy: 0.9472 - val_loss: 0.2048 - val_accuracy: 0.9357\n",
      "Epoch 43/100\n",
      "276500/277200 [============================>.] - ETA: 0s - loss: 0.1391 - accuracy: 0.9483\n",
      "Epoch 00043: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1391 - accuracy: 0.9483 - val_loss: 0.2062 - val_accuracy: 0.9314\n",
      "Epoch 44/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.1357 - accuracy: 0.9492\n",
      "Epoch 00044: val_accuracy did not improve from 0.93571\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1357 - accuracy: 0.9491 - val_loss: 0.2079 - val_accuracy: 0.9343\n",
      "Epoch 45/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.1351 - accuracy: 0.9495\n",
      "Epoch 00045: val_accuracy improved from 0.93571 to 0.93714, saving model to lastIterationV2finnalmodelll_best.h5\n",
      "277200/277200 [==============================] - 22s 80us/sample - loss: 0.1352 - accuracy: 0.9495 - val_loss: 0.2007 - val_accuracy: 0.9371\n",
      "Epoch 46/100\n",
      "276600/277200 [============================>.] - ETA: 0s - loss: 0.1338 - accuracy: 0.9498\n",
      "Epoch 00046: val_accuracy did not improve from 0.93714\n",
      "277200/277200 [==============================] - 22s 79us/sample - loss: 0.1338 - accuracy: 0.9498 - val_loss: 0.2062 - val_accuracy: 0.9329\n",
      "Epoch 00046: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x218ce852e48>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train, # pretrain the wheights # pretrain\n",
    "          epochs=100,\n",
    "          batch_size=100,\n",
    "          shuffle=True, \n",
    "          verbose=1,\n",
    "         validation_data=(x_cross, y_cross),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "350/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 503us/sample - loss: 0.1929 - accuracy: 0.9314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22746010380131856, 0.93142855]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "pretrained_model = tf.keras.models.load_model('lastIterationV2finnalmodelll_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "350/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 477us/sample - loss: 0.1872 - accuracy: 0.9371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2207766910961696, 0.93714285]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(beta_1=0.99), # for avg over 100 values\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "lrate = LearningRateScheduler(step_decay_train)\n",
    "mc = ModelCheckpoint('lastIterationV2finnalmodelll_best_finallllllll.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "\n",
    "callbacks = [es, mc,lrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 277200 samples, validate on 1400 samples\n",
      "Epoch 1/40\n",
      "277000/277200 [============================>.] - ETA: 0s - loss: 0.0553 - accuracy: 0.9794\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.92286, saving model to lastIterationV2finnalmodelll_best_finallllllll.h5\n",
      "277200/277200 [==============================] - 142s 511us/sample - loss: 0.0553 - accuracy: 0.9794 - val_loss: 0.3152 - val_accuracy: 0.9229\n",
      "Epoch 2/40\n",
      "277100/277200 [============================>.] - ETA: 0s - loss: 0.0449 - accuracy: 0.9836\n",
      "Epoch 00002: val_accuracy improved from 0.92286 to 0.92857, saving model to lastIterationV2finnalmodelll_best_finallllllll.h5\n",
      "277200/277200 [==============================] - 140s 506us/sample - loss: 0.0449 - accuracy: 0.9836 - val_loss: 0.3472 - val_accuracy: 0.9286\n",
      "Epoch 3/40\n",
      "277100/277200 [============================>.] - ETA: 0s - loss: 0.0387 - accuracy: 0.9858\n",
      "Epoch 00003: val_accuracy did not improve from 0.92857\n",
      "277200/277200 [==============================] - 138s 498us/sample - loss: 0.0386 - accuracy: 0.9858 - val_loss: 0.3493 - val_accuracy: 0.9214\n",
      "Epoch 4/40\n",
      "277100/277200 [============================>.] - ETA: 0s - loss: 0.0353 - accuracy: 0.9871\n",
      "Epoch 00004: val_accuracy did not improve from 0.92857\n",
      "277200/277200 [==============================] - 138s 497us/sample - loss: 0.0353 - accuracy: 0.9871 - val_loss: 0.3686 - val_accuracy: 0.9243\n",
      "Epoch 5/40\n",
      "277100/277200 [============================>.] - ETA: 0s - loss: 0.0326 - accuracy: 0.9884\n",
      "Epoch 00005: val_accuracy did not improve from 0.92857\n",
      "277200/277200 [==============================] - 137s 493us/sample - loss: 0.0326 - accuracy: 0.9884 - val_loss: 0.3764 - val_accuracy: 0.9214\n",
      "Epoch 6/40\n",
      "  2500/277200 [..............................] - ETA: 2:05 - loss: 0.0400 - accuracy: 0.9850WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-e52e72e28999>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m          validation_data=(x_cross, y_cross), callbacks=callbacks)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 535\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 535\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m   \u001b[0mconstant_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[1;34m(tensor, partial)\u001b[0m\n\u001b[0;32m    790\u001b[0m   \"\"\"\n\u001b[0;32m    791\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    931\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mTensor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrepresentable\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m     \"\"\"\n\u001b[1;32m--> 933\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pretrained_model.fit(x_train, y_train, batch_size = 100,# realtrain\n",
    "          epochs=40,\n",
    "          shuffle=True, \n",
    "          verbose=1,\n",
    "         validation_data=(x_cross, y_cross), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itr = datagen.flow(x_train, y_train, batch_size = x_train.shape[0])\n",
    "# xxx , yyy = itr.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xxx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.save('lastIterationV2finnalmodelll_best_xddddd.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-this will become the gratest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
